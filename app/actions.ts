"use server";

import OpenAI from "openai";
import * as Sentry from "@sentry/nextjs";
import { ContractAnalysisError } from "@/lib/errors";
import { splitIntoChunks } from "@/lib/text-utils";

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

interface AnalysisResult {
  summary: string;
  keyTerms: string[];
  potentialRisks: string[];
  importantClauses: string[];
  recommendations?: string[];
}

interface AnalysisMetadata {
  analyzedAt: string;
  documentName: string;
  modelVersion: string;
  totalChunks: number;
  currentChunk: number;
}

const BATCH_SIZE = 3; // Process 3 chunks at a time for optimal performance/progress balance

async function analyzeChunk(
  chunk: string,
  chunkIndex: number,
  totalChunks: number
): Promise<AnalysisResult> {
  try {
    console.log(`Starting analysis of chunk ${chunkIndex + 1}/${totalChunks}`);
    
    const systemPrompt = "You are a legal expert. Analyze this contract section concisely.";
    const userPrompt = `Section ${chunkIndex + 1}/${totalChunks}:\n${chunk}\n\nProvide JSON with: summary (brief), keyTerms, potentialRisks, importantClauses, recommendations.`;

    console.log(`Making API call for chunk ${chunkIndex + 1}`);
    const response = await withRetry(async () => {
      return await openai.chat.completions.create({
        model: "gpt-3.5-turbo-1106",
        messages: [
          { role: "system", content: systemPrompt },
          { role: "user", content: userPrompt },
        ],
        temperature: 0.3,
        max_tokens: 1000,
        response_format: { type: "json_object" },
      });
    });

    const content = response.choices[0]?.message?.content;
    if (!content) {
      throw new ContractAnalysisError(
        'No analysis generated by AI model',
        'API_ERROR'
      );
    }

    return JSON.parse(content) as AnalysisResult;
  } catch (error) {
    console.error(`Error in analyzeChunk ${chunkIndex + 1}:`, error);
    throw error;
  }
}

async function withRetry<T>(fn: () => Promise<T>, maxAttempts = 3): Promise<T> {
  let lastError: unknown;
  for (let attempt = 1; attempt <= maxAttempts; attempt++) {
    try {
      return await fn();
    } catch (error) {
      lastError = error;
      if (attempt === maxAttempts) break;
      await new Promise(resolve => setTimeout(resolve, Math.pow(2, attempt - 1) * 1000));
    }
  }
  throw lastError;
}

async function processBatch(
  chunks: string[],
  startIndex: number,
  totalChunks: number
): Promise<AnalysisResult[]> {
  return Promise.all(
    chunks.map((chunk, idx) => 
      analyzeChunk(chunk, startIndex + idx, totalChunks)
    )
  );
}

export async function analyzeContract(formData: FormData): Promise<{
  summary: string;
  keyTerms: string[];
  potentialRisks: string[];
  importantClauses: string[];
  recommendations: string[];
  metadata: AnalysisMetadata;
}> {
  try {
    const text = formData.get("text");
    const filename = formData.get("filename");

    if (!text || typeof text !== 'string' || !filename || typeof filename !== 'string') {
      throw new ContractAnalysisError("Invalid input", "INVALID_INPUT");
    }

    const chunks = splitIntoChunks(text);
    if (chunks.length === 0) {
      throw new ContractAnalysisError("Document too short", "INVALID_INPUT");
    }

    const results: AnalysisResult[] = [];
    const metadata: AnalysisMetadata = {
      analyzedAt: new Date().toISOString(),
      documentName: filename,
      modelVersion: "gpt-3.5-turbo-1106",
      totalChunks: chunks.length,
      currentChunk: 0
    };

    // Process chunks in batches
    for (let i = 0; i < chunks.length; i += BATCH_SIZE) {
      const batchChunks = chunks.slice(i, i + BATCH_SIZE);
      const batchResults = await processBatch(batchChunks, i, chunks.length);
      
      results.push(...batchResults);
      metadata.currentChunk = Math.min(i + BATCH_SIZE, chunks.length);

      // This will make the progress updates available to the client
      // through the streaming response
      console.log(JSON.stringify({ 
        type: 'progress',
        current: metadata.currentChunk,
        total: metadata.totalChunks
      }));
    }

    // Merge all results
    const aiSummaries = results.map(r => r.summary).join('\n');
    const finalAnalysis = {
      summary: `Analysis complete. Found ${results.length} key sections.\n\n\nDetailed Analysis:\n${aiSummaries}`,
      keyTerms: Array.from(new Set(results.flatMap(r => r.keyTerms))),
      potentialRisks: Array.from(new Set(results.flatMap(r => r.potentialRisks))),
      importantClauses: Array.from(new Set(results.flatMap(r => r.importantClauses))),
      recommendations: Array.from(new Set(results.flatMap(r => r.recommendations || []))),
      metadata
    };

    return finalAnalysis;

  } catch (error) {
    console.error("Error in analyzeContract:", error);
    throw error;
  }
}