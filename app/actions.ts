"use server";

import OpenAI from "openai";
import * as Sentry from "@sentry/nextjs";
import { ContractAnalysisError } from "@/lib/errors";
import { splitIntoChunks } from "@/lib/text-utils";

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

interface AnalysisResult {
  summary: string;
  keyTerms: string[];
  potentialRisks: string[];
  importantClauses: string[];
  recommendations?: string[];
}

interface AnalysisMetadata {
  analyzedAt: string;
  documentName: string;
  modelVersion: string;
  totalChunks: number;
  currentChunk: number;
}

// Track the state of ongoing analyses
type AnalysisState = {
  results: AnalysisResult[];
  currentChunk: number;
  totalChunks: number;
  startTime: string;
};

const analysisState = new Map<string, AnalysisState>();

async function analyzeChunk(
  chunk: string,
  chunkIndex: number,
  totalChunks: number
): Promise<AnalysisResult> {
  try {
    console.log(`Starting analysis of chunk ${chunkIndex + 1}/${totalChunks}`);
    
    const systemPrompt = "You are a legal expert. Analyze this contract section concisely.";
    const userPrompt = `Section ${chunkIndex + 1}/${totalChunks}:\n${chunk}\n\nProvide JSON with: summary (brief), keyTerms, potentialRisks, importantClauses, recommendations.`;

    const response = await withRetry(async () => {
      return await openai.chat.completions.create({
        model: "gpt-3.5-turbo-1106",
        messages: [
          { role: "system", content: systemPrompt },
          { role: "user", content: userPrompt },
        ],
        temperature: 0.3,
        max_tokens: 1000,
        response_format: { type: "json_object" },
      });
    });

    const content = response.choices[0]?.message?.content;
    if (!content) {
      throw new ContractAnalysisError(
        'No analysis generated by AI model',
        'API_ERROR'
      );
    }

    return JSON.parse(content) as AnalysisResult;
  } catch (error) {
    console.error(`Error in analyzeChunk ${chunkIndex + 1}:`, error);
    throw error;
  }
}

async function withRetry<T>(fn: () => Promise<T>, maxAttempts = 3): Promise<T> {
  let lastError: unknown;
  for (let attempt = 1; attempt <= maxAttempts; attempt++) {
    try {
      return await fn();
    } catch (error) {
      lastError = error;
      if (attempt === maxAttempts) break;
      await new Promise(resolve => setTimeout(resolve, Math.pow(2, attempt - 1) * 1000));
    }
  }
  throw lastError;
}

export async function analyzeContract(formData: FormData) {
  try {
    const text = formData.get("text");
    const filename = formData.get("filename");

    if (!text || typeof text !== 'string' || !filename || typeof filename !== 'string') {
      throw new ContractAnalysisError("Invalid input", "INVALID_INPUT");
    }

    const stateKey = `${filename}-${text.length}`;
    let state = analysisState.get(stateKey);

    // Initialize new analysis
    if (!state) {
      const chunks = splitIntoChunks(text);
      if (chunks.length === 0) {
        throw new ContractAnalysisError("Document too short", "INVALID_INPUT");
      }

      // Return initial state immediately
      const initialState = {
        summary: "Starting analysis...",
        keyTerms: [],
        potentialRisks: [],
        importantClauses: [],
        recommendations: [],
        metadata: {
          analyzedAt: new Date().toISOString(),
          documentName: filename,
          modelVersion: "gpt-3.5-turbo-1106",
          totalChunks: chunks.length,
          currentChunk: 0
        }
      };

      // Initialize state
      state = {
        results: [],
        currentChunk: 0,
        totalChunks: chunks.length,
        startTime: new Date().toISOString()
      };
      analysisState.set(stateKey, state);

      // Start background processing
      (async () => {
        try {
          for (let i = 0; i < chunks.length; i++) {
            const result = await analyzeChunk(chunks[i], i, chunks.length);
            state = analysisState.get(stateKey)!;
            state.results.push(result);
            state.currentChunk = i + 1;
          }
        } catch (error) {
          console.error('Background processing error:', error);
        }
      })();

      return initialState;
    }

    // Return current progress
    let analysis: AnalysisResult;
    if (state.results.length === 0) {
      analysis = {
        summary: "Starting analysis...",
        keyTerms: [],
        potentialRisks: [],
        importantClauses: [],
        recommendations: []
      };
    } else {
      analysis = {
        summary: `Analyzing section ${state.currentChunk} of ${state.totalChunks}...`,
        keyTerms: state.results.flatMap(r => r.keyTerms),
        potentialRisks: state.results.flatMap(r => r.potentialRisks),
        importantClauses: state.results.flatMap(r => r.importantClauses),
        recommendations: state.results.flatMap(r => r.recommendations || [])
      };
    }

    const response = {
      ...analysis,
      metadata: {
        analyzedAt: state.startTime,
        documentName: filename,
        modelVersion: "gpt-3.5-turbo-1106",
        totalChunks: state.totalChunks,
        currentChunk: state.currentChunk
      }
    };

    // If analysis is complete, merge results and clean up
    if (state.currentChunk === state.totalChunks) {
      const mergedAnalysis = {
        summary: `Analysis complete. Found ${state.results.length} key sections.`,
        keyTerms: [...new Set(state.results.flatMap(r => r.keyTerms))],
        potentialRisks: [...new Set(state.results.flatMap(r => r.potentialRisks))],
        importantClauses: [...new Set(state.results.flatMap(r => r.importantClauses))],
        recommendations: [...new Set(state.results.flatMap(r => r.recommendations || []))],
        metadata: response.metadata
      };
      analysisState.delete(stateKey);
      return mergedAnalysis;
    }

    return response;

  } catch (error) {
    console.error("Error in analyzeContract:", error);
    throw error;
  }
}